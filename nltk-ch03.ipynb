{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 3 Processing Raw Text by DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Text from the Web and from Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electronic Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1176965\n",
      "The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "print(type(raw))\n",
    "print(len(raw))\n",
    "print(raw[1:74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "27317\n",
      "孫子曰：兵者，國之大事，死生之地，存亡之道，不可不察也。\n"
     ]
    }
   ],
   "source": [
    "url2 = \"http://www.gutenberg.org/files/23864/23864-0.txt\"\n",
    "response2 = request.urlopen(url2)\n",
    "raw2 = response2.read().decode('utf8')\n",
    "print(type(raw2))\n",
    "print(len(raw2))\n",
    "print(raw2[572:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "257726\n",
      "['Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by', 'Fyodor', 'Dostoevsky']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[1:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.']\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(text[1021:1059])\n",
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "['孫', '子', '曰', '：', '兵', '者', '，', '國', '之', '大', '事', '，', '死', '生', '之', '地', '，', '存', '亡', '之', '道', '，', '不', '可', '不', '察', '也', '。']\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text2 = nltk.Text(raw2)\n",
    "print(type(text2))\n",
    "print(text2[572:600])\n",
    "print(text2.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336\n",
      "1157810\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "print(raw.find(\"PART I\"))\n",
    "print(raw.rfind(\"End of Project Gutenberg\"))\n",
    "print(raw.find(\"PART 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n",
      "8174\n"
     ]
    }
   ],
   "source": [
    "print(raw2.find(\"孫子\"))\n",
    "print(raw2.rfind(\"三軍之所恃而動也\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n"
     ]
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "print(html[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose\n"
     ]
    }
   ],
   "source": [
    "print(html[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-', 'Country', 'Profiles', 'In', 'Depth', '--', '--', '--', '--', '--', '--', '-', 'Programmes', '--', '--', '--', '--', '--', '--', '-', 'SERVICES', 'Daily', 'E-mail', 'News', 'Ticker', 'Mobile/PDAs', '--', '--', '--', '--', '--', '--', '-', 'Text', 'Only', 'Feedback', 'Help', 'EDITIONS', 'Change', 'to', 'UK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lab\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file c:\\users\\lab\\appdata\\local\\programs\\python\\python36-32\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "print(text.concordance('gene'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text2 = nltk.Text(raw2)\n",
    "print(text2.concordance('孫子'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Search Engine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blonde hair; Jonathan Rees; n't disappear; blondes would; blondes may\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Log\n",
      "13\n",
      "Accentuate the negative\n",
      "<p>A curious case of a forced-choice sentence-completion question on a\n",
      "['A', 'curious', 'case', 'of', 'a', 'forced-choice', 'sentence-completion', 'question', 'on', 'a', 'ninth-grade', 'exam', 'at', 'a', 'high', 'schoo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lab\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file c:\\users\\lab\\appdata\\local\\programs\\python\\python36-32\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "print(llog['feed']['title'])\n",
    "print(len(llog.entries))\n",
    "post = llog.entries[2]\n",
    "print(post.title)\n",
    "content = post.content[0].value\n",
    "print(content[:70])\n",
    "raw = BeautifulSoup(content).get_text()\n",
    "print(word_tokenize(raw[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'langconv.py',\n",
       " 'nltk-ch01.ipynb',\n",
       " 'nltk-ch02.ipynb',\n",
       " 'nltk-ch03.ipynb',\n",
       " 'text_proc.py',\n",
       " 'text_proc.pyc',\n",
       " 'zh_wiki.py',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def plural(word):\n",
      "    if word.endswith('y'):\n",
      "        return word[:-1] + 'ies'\n",
      "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
      "        return word + 'es'\n",
      "    elif word.endswith('an'):\n",
      "        return word[:-2] + 'en'\n",
      "    else:\n",
      "        return word + 's'\n",
      "\n",
      "def lower_word_set(text):\n",
      "    return set(w.lower() for w in text if w.isalpha())\n",
      "\n",
      "def diff_word_set(text, ref):\n",
      "    t = lower_word_set(text)\n",
      "    r = lower_word_set(ref)\n",
      "    return t-r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('text_proc.py')\n",
    "raw = f.read()\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def plural(word):\n",
      "if word.endswith('y'):\n",
      "return word[:-1] + 'ies'\n",
      "elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
      "return word + 'es'\n",
      "elif word.endswith('an'):\n",
      "return word[:-2] + 'en'\n",
      "else:\n",
      "return word + 's'\n",
      "\n",
      "def lower_word_set(text):\n",
      "return set(w.lower() for w in text if w.isalpha())\n",
      "\n",
      "def diff_word_set(text, ref):\n",
      "t = lower_word_set(text)\n",
      "r = lower_word_set(ref)\n",
      "return t-r\n"
     ]
    }
   ],
   "source": [
    "f = open('text_proc.py', 'r')\n",
    "for line in f:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\n",
      "\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\n",
      "\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\n",
      "now.  He was ever du\n"
     ]
    }
   ],
   "source": [
    "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    "raw = open(path, 'r').read()\n",
    "print(raw[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text from PDF, MSWord and other Binary Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text: hello world\n",
      "You typed 2 words.\n"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter some text: \")\n",
    "print(\"You typed\", len(word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "raw = open('text_proc.py').read()\n",
    "print(type(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "words = [w.lower() for w in tokens]\n",
    "print(type(words))\n",
    "vocab = sorted(set(words))\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('blog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings: Text Processing at the Lowest Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty Python\n",
      "Monty Python's Flying Circus\n",
      "Monty Python's Flying Circus\n"
     ]
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "print(monty)\n",
    "circus = \"Monty Python's Flying Circus\"\n",
    "print(circus)\n",
    "circus = 'Monty Python\\'s Flying Circus'\n",
    "print(circus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n",
      "Rough winds do shake the darling buds of May,And Summer's lease hath all too short a date:\n",
      "Shall I compare thee to a Summer's day?\n",
      "    Thou are more lovely and more temperate:\n",
      "Rough winds do shake the darling buds of May,\n",
      "    And Summer's lease hath all too short a date:\n"
     ]
    }
   ],
   "source": [
    "couplet = \"Shall I compare thee to a Summer's day?\"\\\n",
    "    \"Thou are more lovely and more temperate:\"\n",
    "print(couplet)\n",
    "couplet = (\"Rough winds do shake the darling buds of May,\"\n",
    "    \"And Summer's lease hath all too short a date:\")\n",
    "print(couplet)\n",
    "couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
    "    Thou are more lovely and more temperate:\"\"\"\n",
    "print(couplet)\n",
    "couplet = '''Rough winds do shake the darling buds of May,\n",
    "    And Summer's lease hath all too short a date:'''\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veryveryvery\n",
      "veryveryvery\n"
     ]
    }
   ],
   "source": [
    "print('very' + 'very' + 'very')\n",
    "print('very' * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monty Python\n",
      "Monty PythonHoly Grail\n",
      "Monty Python Holy Grail\n",
      "Monty Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "print(monty)\n",
    "grail = 'Holy Grail'\n",
    "print(monty + grail)\n",
    "print(monty, grail)\n",
    "print(monty, \"and the\", grail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Individual Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "t\n",
      " \n",
      "n\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(monty[0])\n",
    "print(monty[3])\n",
    "print(monty[5])\n",
    "print(monty[-1])\n",
    "print(monty[-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y "
     ]
    }
   ],
   "source": [
    "sent = 'colorless green ideas sleep furiously'\n",
    "for char in sent:\n",
    "    print(char, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]\n",
      "['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w', 'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "raw = gutenberg.raw('melville-moby_dick.txt')\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "print(fdist.most_common(5))\n",
    "print([char for (char, count) in fdist.most_common()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyth\n",
      "Monty\n",
      "Monty\n",
      "Python\n"
     ]
    }
   ],
   "source": [
    "print(monty[6:10])\n",
    "print(monty[-12:-7])\n",
    "print(monty[:5])\n",
    "print(monty[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found \"thing\"\n"
     ]
    }
   ],
   "source": [
    "phrase = 'And now for something completely different'\n",
    "if 'thing' in phrase:\n",
    "    print('found \"thing\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " monty.find('Python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More operations on strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Difference between Lists and Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "George\n",
      "Wh\n",
      "['John', 'Paul']\n",
      "Who knows? I don't\n",
      "['John', 'Paul', 'George', 'Ringo', 'Brian']\n"
     ]
    }
   ],
   "source": [
    "query = 'Who knows?'\n",
    "beatles = ['John', 'Paul', 'George', 'Ringo']\n",
    "print(query[2])\n",
    "print(beatles[2])\n",
    "print(query[:2])\n",
    "print(beatles[:2])\n",
    "print(query + \" I don't\")\n",
    "print(beatles + ['Brian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Lennon', 'Paul', 'George']\n"
     ]
    }
   ],
   "source": [
    "beatles[0] = \"John Lennon\"\n",
    "del beatles[-1]\n",
    "print(beatles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
